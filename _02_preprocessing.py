
'''
This script takes the data frame of raw notes generated by the previous script and does the following:
1.  Removes medication lists
2.  Joins multiword expressions
3.  "windows" notes:
    initilialize empty list of MRNs who won't be eligible for the next 6 months
    for each month
        define eligible notes as people who haven't had an eligible note in the last 6 months but are otherwise eligible
        add those people to the 6-month list, along with the month in which they were added
        take those notes, and concatenate them with the last 6 months of notes
        process the combined note, and output it
'''


import pandas as pd
import matplotlib.pyplot as plt
from flashtext import KeywordProcessor
import pickle
import re
import multiprocessing as mp
import time
import numpy as np

datadir = "/Users/crandrew/projects/GW_PAIR_frailty_classifier/data/"
outdir = "/Users/crandrew/projects/GW_PAIR_frailty_classifier/output/"
figdir = "/Users/crandrew/projects/GW_PAIR_frailty_classifier/figures/"

# preferences
pd.options.display.max_rows = 4000
pd.options.display.max_columns = 4000

# load the raw notes:
df = pd.read_pickle(f"{outdir}raw_notes_df.pkl")

'''
Removing medication lists
'''
def med_list_span_finder(x):
    x = x.lower()
    # convert all non-breaking space to regular space
    x = re.sub("\xa0", " ", x)
    # medication_list_finder:
    start_phrases = ["medication\n", "medications\n", "prescriptions\n", "medication:", "medications:", "prescriptions:",
                     "prescriptions on file"]
    stop_phrases = ["allergies\n","allergies:", "allergy list\n", "allergy list:",
                    "-------", "\n\n\n\n",
                    "active medical_problems", "active medical problems", "patient active",
                    "past surgical history", "past_surgical_history",
                    "past medical history", "past_medical_history",
                    "review of symptoms", "review_of_symptoms",
                    "review of systems", "review_of_systems",
                    "family history", "family_history",
                    "social history", "social_history", "social hx:",
                    "physical exam:", "physical_examination", "physical examination",
                    "history of present illness",
                    "vital signs",
                    "\ni saw ", " i saw ",
                    "\npe:",
                    "current issues",
                    "history\n", "history:"]
    cutter_phrases = ["by_mouth", "by mouth"]
    keeper_phrases = ["assessment"]
    st_idx = [m.start() for m in re.finditer("|".join(start_phrases), x)]
    stop_idx = [m.start() for m in re.finditer("|".join(stop_phrases), x)]

    # for each instance of pmh, define a span up to the subseqent.  cut that span at the next instance of 'past surgical history'
    spans = []
    try:
        for j in range(len(st_idx)):
            startpos = st_idx[j]
            endpos = next((x for x in stop_idx if x > startpos), None)
            if endpos is not None:
                if j < (len(st_idx)-1): # if j is not the last element being iterated over:
                    if endpos < st_idx[j+1]: # if the end position is before the next start position
                        # implicity, if the above condition doesn't hold, nothing will get appended into spans
                        # and it'll go to the next j
                        span = x[startpos:endpos]
                        # things that it must have to get cut
                        if any(re.finditer("|".join(cutter_phrases), span)):
                            # thing that if it has it can't be cut
                            if not any(re.finditer("|".join(keeper_phrases), span)):
                                extra = x[endpos:(endpos+200)]
                                spans.append((startpos, endpos))
                elif j == (len(st_idx)-1): # if it's the last start index, there better be a following stop index
                    span = x[startpos:endpos]
                    # things that it must have to get cut
                    if any(re.finditer("|".join(cutter_phrases), span)):
                        # thing that if it has it can't be cut
                        if not any(re.finditer("|".join(keeper_phrases), span)):
                            extra = x[endpos:(endpos+200)]
                            spans.append((startpos, endpos))
        return spans
    except Exception as e:
        return e


def cut_medlists(x):
    spans = med_list_span_finder(x)
    for i in spans:
        cut = x[i[0]:i[1]]
        x = re.sub(re.escape(cut), "---medlist_was_here_but_got_cut----", x)
    return x

pool = mp.Pool(mp.cpu_count())
start = time.time()
cut_notes = pool.map(cut_medlists, df.NOTE_TEXT, chunksize=1)
print(time.time() - start)
pool.close()

df['NOTE_TEXT'] = cut_notes
'''
initialize two empty data frames with patient ID and time columns.  
    - the first is the windower
    - the second is the running list of note files to generate
loop through months.  at each month:
    - drop people from the windower if they were added more than 6 months ago
    - add people to a temporary list if they are not in the windower and have a note that month
    - append the temporary list to the running list, and to the windower

'''
# create month since jan 2018 variable
df = df[df.ENC_DATE.dt.year>=2017]
df['month'] = df.ENC_DATE.dt.month + (df.ENC_DATE.dt.year - min(df.ENC_DATE.dt.year))*12
# create empty dfs
windower = pd.DataFrame(columns=["PAT_ID", "month"])
running_list = pd.DataFrame(columns=["PAT_ID", "month"])

months = [i for i in range(min(df['month']), max(df['month']) + 1)]

for m in months[12:]:
    windower = windower[(m - windower['month']) < 6]
    tmp = df[(df["month"] == m) & (~df['PAT_ID'].isin(windower['PAT_ID']))][
        ["PAT_ID", "month"]].drop_duplicates()
    windower = pd.concat([windower, tmp], axis=0, ignore_index=True)
    running_list = pd.concat([running_list, tmp], axis=0, ignore_index=True)

# plot notes per month
notes_by_month = running_list.month.value_counts().sort_index().reset_index(drop = True)
f = plt.figure()
axes = plt.gca()
axes.set_ylim([0, max(notes_by_month) + 100])
plt.plot(notes_by_month.index.values, notes_by_month, "o")
plt.plot(notes_by_month.index.values, notes_by_month)
plt.xlabel("Months since Jan 2018")
plt.ylabel("Number of notes")
# plt.show()
plt.figure(figsize=(8, 8))
f.savefig(f'{figdir}pat_per_month.pdf')


'''
armed with the running list of patient IDs, go through the note text, month by month, and concatenate all notes from 
that patient.  join MWEs while at it.
'''
mwe_dict = pickle.load(open("/Users/crandrew/projects/pwe/output/mwe_dict.pkl", 'rb'))
macer = KeywordProcessor()
macer.add_keywords_from_dict(mwe_dict)


def identify_mwes(s, macer):
    return macer.replace_keywords(s)


joiner = "\n--------------------------------------------------------------\n"




def proc(j):
    try:
        pi, mi = running_list.PAT_ID[j], running_list.month[j]  # the "+12 is there because the running list started in 2018"
        # slice the df
        ni = df[(df.PAT_ID == pi) &
                ((mi - df.month) < 6) &
                ((mi - df.month) >= 0)]
        ni = ni.sort_values(by=["ENC_DATE"], ascending=False)
        # process the notes
        comb_notes = [identify_mwes(i, macer) for i in ni.NOTE_TEXT]
        comb_string = ""
        for i in list(range(len(comb_notes))):
            comb_string = comb_string + joiner + str(ni.ENC_DATE.iloc[i]) + \
                          joiner + comb_notes[i]
        # lose multiple newlines
        comb_string = re.sub("\n+", "\n", comb_string)
        # count words
        wds = re.split(" |\n", comb_string)
        wds = [i.lower() for i in wds if i != ""]
        # join the diagnoses
        dxs = list(set((','.join(ni.dxs[~ni.dxs.isnull()].tolist())).split(",")))
        dxs.sort()
        comb_note_dict_i = dict(PAT_ID=ni.PAT_ID.iloc[0],
                                LATEST_TIME=ni.ENC_DATE.iloc[0],
                                CSNS=",".join(ni.CSN.astype(str).to_list()),
                                dxs = dxs,
                                n_comorb = len(dxs),
                                n_notes=ni.shape[0],
                                n_words=len(wds),
                                u_words=len(set(wds)),
                                combined_notes=comb_string)
        return comb_note_dict_i
    except Exception as e:
        return dict(which = j, error = e)


pool = mp.Pool(processes=mp.cpu_count())
start = time.time()
dictlist = pool.map(proc, range(running_list.shape[0]), chunksize=1)
print(time.time() - start)
pool.close()

errs = [i for i in dictlist if "error" in i.keys()]
assert len(errs) == 0

ds = dictlist
d = {}
for k in dictlist[1].keys(): # dict 1 is arbitrary -- it's just pulling the keys
    d[k] = tuple(d[k] for d in ds)
conc_notes_df = pd.DataFrame(d)

# looks for words in the text
low_prob_words = ['gym', 'exercise', 'breathing', 'appetite', 'eating', 'getting around', 'functional status',
                  'PO intake', 'getting around', 'walking', 'running', 'independent']
high_prob_words = ['PO intake', 'weight loss', 'appetite', 'frail', 'frailty', 'weakness', 'feels weak', 'unsteady',
                   'recent fall', 'getting around', 'severe dyspnea', 'functional impairment', 'difficulty walking',
                   'difficulty breathing', 'getting in the way', 'exercise', 'Breathless', 'short of breath',
                   'wheezing', 'delirium', 'dementia', 'incontinence', 'do not resuscitate', 'walker', 'wheelchair',
                   'malnutrition', 'boost']
low_prob_words = [identify_mwes(i, macer) for i in low_prob_words]
high_prob_words = [identify_mwes(i, macer) for i in high_prob_words]

low_prob_regex = '|'.join(low_prob_words)
high_prob_regex = '|'.join(high_prob_words)
# append the hp and lp columns
lp = conc_notes_df.combined_notes.str.contains(low_prob_regex)
hp = conc_notes_df.combined_notes.str.contains(high_prob_regex)
conc_notes_df['highprob'] = hp.values
conc_notes_df['lowprob'] = lp.values



conc_notes_df.to_pickle(f'{outdir}conc_notes_df.pkl')
# conc_notes_df = pd.read_pickle(f'{outdir}conc_notes_df.pkl')

conc_notes_df['month'] = conc_notes_df.LATEST_TIME.dt.month + (
        conc_notes_df.LATEST_TIME.dt.year - min(conc_notes_df.LATEST_TIME.dt.year)) * 12
months = list(set(conc_notes_df.month))
months.sort()

def plotfun(var, yaxt, q=False):
    f = plt.figure()
    axes = plt.gca()
    if q:
        qvec = [np.quantile(conc_notes_df[var][conc_notes_df.month == i], [.25, .5, .75]).reshape(1, 3) for i in months]
        qmat = np.concatenate(qvec, axis=0)
        axes.set_ylim([0, np.max(qmat)])
        plt.plot(months, qmat[:, 1], "C1", label="median")
        plt.plot(months, qmat[:, 0], "C2", label="first quartile")
        plt.plot(months, qmat[:, 2], "C2", label="third quartile")
    else:
        sdvec = [np.std(conc_notes_df[var][conc_notes_df.month == i]) for i in months]
        muvec = [np.mean(conc_notes_df[var][conc_notes_df.month == i]) for i in months]
        axes.set_ylim([0, max(np.array(muvec) + np.array(sdvec))])
        plt.plot(months, muvec, "C1", label="mean")
        plt.plot(months, np.array(muvec) + np.array(sdvec), "C2", label="+/- 1 sd")
        plt.plot(months, np.array(muvec) - np.array(sdvec), "C2")
    plt.xlabel("Months since Jan 2018")
    plt.ylabel(yaxt)
    axes.legend()
    # plt.show()
    plt.figure(figsize=(8, 8))
    f.savefig(f'{figdir}{var}.pdf')


plotfun("n_notes", "Number of notes per combined note")
plotfun("n_words", "Number of words per combined note", q=True)
plotfun("u_words", "Number of unique words per combined note", q=True)


# numbers of words by number of conc notes
f, ax = plt.subplots()
nnn = list(set(conc_notes_df.n_notes))
pltlist = [conc_notes_df.u_words[conc_notes_df.n_notes == i] for i in nnn if i < 15]
ax.set_title('Unique words by number of concatenated notes')
ax.boxplot(pltlist)
plt.xlabel("Number of notes concatenated together")
plt.ylabel("Number of unique words")
plt.figure(figsize=(8, 8))
f.savefig(f'{figdir}nnotes_by_uwords.pdf')

# pull some random notes
np.random.seed(8675309)
kind = "lp"
for i in conc_notes_df.month.unique():
    if kind == "lp":
        sampdf = conc_notes_df[(conc_notes_df.month == i) &
                               (conc_notes_df.lowprob == True) &
                               (conc_notes_df.highprob == False) &
                               (conc_notes_df.n_comorb <5)]
        samp = np.random.choice(sampdf.shape[0], 1)
        towrite = sampdf.combined_notes.iloc[int(samp)]
        fi = f"batch_01_m{sampdf.month.iloc[int(samp)]}_{sampdf.PAT_ID.iloc[int(samp)]}.txt"
        with open(f'{outdir}/notes_output/batch_01/{fi}', "w") as f:
            f.write(towrite)
        kind = "hp"
    elif kind == "hp":
        sampdf = conc_notes_df[(conc_notes_df.month == i) &
                               (conc_notes_df.lowprob == False) &
                               (conc_notes_df.highprob == True) &
                               (conc_notes_df.n_comorb >15)]
        samp = np.random.choice(sampdf.shape[0], 1)
        towrite = sampdf.combined_notes.iloc[int(samp)]
        fi = f"batch_01_m{sampdf.month.iloc[int(samp)]}_{sampdf.PAT_ID.iloc[int(samp)]}.txt"
        with open(f'{outdir}/notes_output/batch_01/{fi}', "w") as f:
            f.write(towrite)
        kind = "lp"




