import copyimport warningsfrom configargparse import ArgParserimport reimport osimport pandas as pdimport tensorflow as tffrom utils.prefit import make_modelfrom utils.misc import (read_pickle, sheepish_mkdir,                        send_message_to_slack, entropy, write_txt)from utils.constants import SENTENCE_LENGTH, TAGS, STR_VARNAMESimport spacyfrom _05_ingest_to_sentence import set_custom_boundariesimport numpy as npimport datetimeclass BatchPredictor():    def __init__(self,digits, batchstring):            self.outdir = f"{os.getcwd()}/output/"        self.datadir = f"{os.getcwd()}/data/"        self.ALdir = f"{self.outdir}saved_models/AL{batchstring}/"        self.digits = digits        self.batchstring = batchstring        sheepish_mkdir(f"{self.ALdir}final_model/preds")                #################        # anti-clobber: look for a token or output, write a token if not found        # if a token or a pred is found, write something to self that short-circuits other methods        stuff_in_preds_dir = os.listdir(f"{self.ALdir}final_model/preds")        stuff_with_digits =[i for i in stuff_in_preds_dir if i.split('_')[1][:2] == self.digits]        if len(stuff_with_digits)>0:            self.exit_to_avoid_clobbering = True            return        else:            write_txt("I am a token.  Why must we anthropomorphize everything?",                       f"{self.ALdir}final_model/preds/token_{self.digits}.txt")            self.exit_to_avoid_clobbering = False            #################            # load data and subset            self.df = pd.read_csv(f"{self.ALdir}processed_data/full_set/full_df.csv", index_col = 0)                already_pulled_PIDs = []            for root, dirs, files in os.walk(f"{self.outdir}notes_output"):                for file in files:                    already_pulled_PIDs.append(file)            already_pulled_PIDs  = [i for i in already_pulled_PIDs if ('batch' in i or 'AL' in i) and '.txt' in i]                    already_pulled_PIDs = list(set([re.sub(".txt", "", i.split("_")[-1]) for i in already_pulled_PIDs]))                    allnotes = pd.read_pickle(f"{self.outdir}conc_notes_df.pkl")            allnotes = allnotes.loc[(allnotes.LATEST_TIME < "2019-01-01") &                                    (~allnotes.PAT_ID.isin(self.df.PAT_ID)) &  # remove PAT IDs that are already in training set                                    (~allnotes.PAT_ID.isin(already_pulled_PIDs))] # remove PAT IDs that have ever been pulled for a test batch            allnotes = allnotes.loc[allnotes.PAT_ID.apply(lambda x: x[-2:]) == self.digits]            allnotes.insert(0, "uid", allnotes.PAT_ID + "_" + allnotes.LATEST_TIME.dt.month.astype(str))            strdat = pd.read_csv(f"{self.outdir}structured_data_merged_cleaned.csv", index_col = 0)               strdat.insert(0, "uid", strdat.PAT_ID + "_" + strdat.month.astype(str))            self.allnotes = allnotes.loc[allnotes.uid.isin(strdat.uid)]            self.strdat = strdat.loc[strdat.uid.isin(self.allnotes.uid)]            self.str_varnames = [i for i in self.df.columns if re.match("pca[0-9]",i)]            self.scalers = read_pickle(f"{self.ALdir}/processed_data/full_set/full_sklearn_dict.pkl")            ####################            # spacy            self.sci_nlp = spacy.load("en_core_sci_md", disable=['tagger', 'ner'])            self.sci_nlp.add_pipe(set_custom_boundaries, before="parser",                                  name='set_custom_boundaries')                    #######################            # Load the model and vectorizer            mod_dict = read_pickle(f"{self.ALdir}/final_model/model_final_{batchstring}.pkl")            self.model, self.vectorizer = make_model(emb_path = f"{self.datadir}w2v_oa_all_300d.bin",                                                     sentence_length = SENTENCE_LENGTH,                                                     meta_shape = len(self.str_varnames),                                                     tags = TAGS,                                                     train_sent = self.df['sentence'],                                                     l1_l2_pen = mod_dict['config']['l1_l2_pen'],                                                     n_units = mod_dict['config']['n_units'],                                                     n_dense = mod_dict['config']['n_dense'],                                                     dropout = mod_dict['config']['dropout'])            self.model.compile(loss='categorical_crossentropy',                          optimizer=tf.keras.optimizers.Adam(1e-4))            self.model.set_weights(mod_dict['weights'])    def sentencize(self, note, return_sentences = False):        res = self.sci_nlp(note)        sentences = []        s = []        for i, tok in enumerate(res):            if (bool(tok.is_sent_start)) & (len(s) > 0):                newsent = " ".join(s)                sentences.append(newsent)                s = []            s.append(str(tok))        sentences.append(" ".join(s))        text = self.vectorizer(np.array([[s] for s in sentences]))        if return_sentences:            return text, sentences        else:            return text    def process_structured(self, row, nsent):        str_mat = pd.concat([row for i in range(nsent)])[STR_VARNAMES]        str_mat = self.scalers['imputer'].transform(str_mat)        str_mat = self.scalers['scaler_in'].transform(str_mat)        str_mat = self.scalers['pca'].transform(str_mat)        str_mat = self.scalers['scaler_out'].transform(str_mat)        struc = tf.convert_to_tensor(str_mat, dtype = 'float32')        return struc    def predict(self, ii):        text = self.sentencize(self.allnotes.combined_notes.iloc[ii])        struc = self.process_structured(self.strdat.loc[self.strdat.uid == self.allnotes.uid.iloc[ii]],                                         nsent = text.shape[0])        pred = self.model.predict([text, struc])        return pred            def get_H_stats(self, pred):        '''pred is a list of predictions for each aspect'''        nsent = pred[0].shape[0]        hmat = np.stack([entropy(i) for i in pred])        average_entropy = np.nanmean(hmat)        max_entropy = np.nanmax(hmat)        hflat = hmat.flatten()        average_entropy_above_median = np.mean(hflat[hflat>np.nanmedian(hflat)])        hcut = copy.deepcopy(hmat)        for i in range(hcut.shape[0]):            hcut[i,:][hcut[i,:]<np.nanmedian(hcut[i,:])] = np.nan        hcut = np.nanmax(hcut, axis = 0)        average_entropy_above_median_notewise = np.nanmean(hcut)        return dict(nsent = nsent,                    average_entropy=average_entropy,                    max_entropy=max_entropy,                    average_entropy_above_median=average_entropy_above_median,                    average_entropy_above_median_notewise=average_entropy_above_median_notewise)    def loop_predict(self):        if self.exit_to_avoid_clobbering == True:            return        else:            outlist = []            for ii in range(len(self.allnotes)):            # for ii in range(3):                pred = self.predict(ii)                with warnings.catch_warnings():                    warnings.simplefilter("ignore")                    out = self.get_H_stats(pred)                out['uid'] = self.allnotes.uid.iloc[ii]                # print(out)                outlist.append(out)            dfout = pd.DataFrame(outlist)            dfout.to_json(f"{self.ALdir}final_model/preds/preds_{self.digits}.json.bz2")            # os.remove(f"{self.ALdir}final_model/preds/token_{self.digits}.txt") # remove the token, just to keep tidy            return dfout    def pull_best_notes(self, N_to_retrieve):        predfiles = os.listdir(f'{self.ALdir}/final_model/preds/')        predfiles = [i for i in predfiles if "json.bz2" in i]        preds = pd.concat([pd.read_json(f"{self.ALdir}/final_model/preds/{i}") for i in predfiles])        best = preds.loc[(preds.nsent>20) & (~preds.average_entropy.isna())]        # deal with multiple PAT IDs:  keep the row with the max entropy        best = best.reset_index(drop = True)        best['PAT_ID'] = best.uid.apply(lambda x: x.split("_")[0])        dups = list(best.PAT_ID.value_counts()[best.PAT_ID.value_counts()>1].index)        for d in dups:            duprows = best.loc[best.PAT_ID == d]            droppers = duprows.loc[duprows.average_entropy_above_median_notewise != max(duprows.average_entropy_above_median_notewise)].index            best.drop(droppers, inplace = True)        best = best.sort_values('average_entropy_above_median_notewise').tail(N_to_retrieve)        assert best.PAT_ID.nunique() == N_to_retrieve        # merge with other data        allnotes = pd.read_pickle(f"{self.outdir}conc_notes_df.pkl")        allnotes = allnotes.loc[(allnotes.LATEST_TIME < "2019-01-01") &                                (~allnotes.PAT_ID.isin(self.df.PAT_ID))] # remove PAT IDs that have ever been pulled for a test batch        allnotes.insert(0, "uid", allnotes.PAT_ID + "_" + allnotes.LATEST_TIME.dt.month.astype(str))        strdat = pd.read_csv(f"{self.outdir}structured_data_merged_cleaned.csv", index_col = 0)           strdat.insert(0, "uid", strdat.PAT_ID + "_" + strdat.month.astype(str))        allnotes = allnotes.loc[allnotes.uid.isin(strdat.uid)]        strdat = strdat.loc[strdat.uid.isin(allnotes.uid)]        best = best.merge(allnotes).merge(strdat)                        diags = pd.read_pickle(f"{self.datadir}diagnosis_df.pkl")        diags = diags.loc[diags.PAT_ID.isin(best.PAT_ID)]        di = []        for i in best.PAT_ID.unique():            d = diags.loc[(diags.PAT_ID == i)].copy()            d['timediff'] = (best.LATEST_TIME.loc[best.PAT_ID==i].iloc[0] - d['dx_date'])            qdx = d.loc[(d.timediff <= datetime.timedelta(days = 365)) &                        (d.timediff >= datetime.timedelta(days = 0)), 'icd10']                        qdx = ",".join(list(qdx.unique()))            di.append(dict(PAT_ID = i, qual_dx = qdx))        tomerge = pd.DataFrame(di)        best = tomerge.merge(best)        self.best = best        best.to_csv(f'{self.ALdir}/final_model/preds/next_batch.csv')        assert best.shape[0] == N_to_retrieve        return best    def write_notes(self):        sheepish_mkdir(f"{self.outdir}/notes_output/AL{self.batchstring}")        for i in range(self.best.shape[0]):            fn = f"AL{self.batchstring}_m{self.best.month.iloc[i]}_{self.best.PAT_ID.iloc[i]}.txt"            write_txt(self.best.combined_notes.iloc[i],                      f"{self.outdir}/notes_output/AL{self.batchstring}/{fn}")                # pd.options.display.max_rows = 4000# pd.options.display.max_columns = 4000# xx = BatchPredictor(digits = 'all', batchstring = '03')# best = xx.pull_best_notes(50)# xx.write_notes()# best.iloc[:10, :10]# best.head()# best.monthdef main():    p = ArgParser()    p.add("-b", "--batchstring", help="the batch number", type=str)    p.add("--pull_notes", default = 0, type=int)    options = p.parse_args()    batchstring = options.batchstring    assert batchstring is not None    outdir = f"{os.getcwd()}/output/"    ALdir = f"{outdir}saved_models/AL{batchstring}/"    sheepish_mkdir(f"{ALdir}final_model/preds")    if options.pull_notes>0:        predictor = BatchPredictor(digits = "all", batchstring = batchstring)        _ = predictor.pull_best_notes(options.pull_notes)        predictor.write_notes()    else:        digits_vec = np.random.choice([str(i)[1:] for i in range(100,200)], 100, replace = True)        for digits in digits_vec:            try:                predictor = BatchPredictor(digits = digits, batchstring = batchstring)                _ = predictor.loop_predict()                            except Exception as e:                print(e)                ip = os.popen("/sbin/ip -o -4 addr list eth0 | awk '{print $4}' | cut -d/ -f1").read()                send_message_to_slack(f"problem processing {digits} at {ip}")if __name__ == "__main__":    main()# best.head()# import matplotlib.pyplot as plt# sheepish_mkdir(f'{xx.ALdir}/final_model/predfigs/')# for ii in range(len(best)):#     try:#         print(ii)#         text, sentences = xx.sentencize(best.combined_notes.iloc[ii], return_sentences = True)#         struc = xx.process_structured(best.loc[[ii], STR_VARNAMES], text.shape[0])#         preds = xx.model.predict([text, struc])#         pmat = np.concatenate([i for i in preds], axis = 1)        #         fig, ax = plt.subplots(figsize = (30, len(sentences)*.5))#         im = ax.imshow(pmat)#         # cbar = ax.figure.colorbar(im, ax=ax)#         for x in [2.5, 5.5, 8.5]:#             ax.axvline(x, color = 'red')    #         ax.set_xticks(list(range(12)))    #         ax.set_xticklabels(['-', 'F', '+','-', 'M', '+','-', 'N', '+','-', 'R', '+'])    #         ax.set_yticks(np.arange(len(sentences)))#         ax.set_yticklabels(sentences)        #         fig.tight_layout()#         fig.savefig(f'{xx.ALdir}/final_model/predfigs/predfig_{ii}.pdf')#     except Exception as e:#         print(e)# len(sentences)# text.shape#     np.concatenate([preds[i], np.expand_dims(H, -1)], axis = 1)# import matplotlib.pyplot as plt# fig, ax = plt.subplots()# im = ax.imshow(np.expand_dims(np.array(hdf.loc[:,0:21]),-1))# # We want to show all ticks...# ax.set_yticks(np.arange(len(hdf.sent)))# # ... and label them with the respective list entries# ax.set_yticklabels(hdf.sent)# ax.set_xticks(list(range(22)))# ax.set_xticklabels(['Emotional', '','',#                     'Existential', '','','',#                     'Goals of Care', '','','','','','',#                     'Physical','','','',#                     'Social','','',''], rotation = 'vertical')# # # Loop over data dimensions and create text annotations.# # for i in range(len(hdf.sent)):# #     text = ax.text(1, i, np.round(hdf.sum_ent.iloc[i], 2),# #                    ha="center", va="center", color="b")# fig.tight_layout()